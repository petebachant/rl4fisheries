# template for using sb3_zoo hyperparameter yamls

# algo overall
algo: "RPPO"
total_timesteps: 35000000

additional_imports: ["torch"]

# env overall
env_id: "AsmEnv"
config: 
    observation_fn_id: 'observe_1o'
    n_observs: 1
n_envs: 12

# io
repo: "cboettig/rl-ecology"
save_path: "../saved_agents"

# # MINIMAL CONFIG
# id: "minimal"
# algo_config:
#     policy: 'MlpLstmPolicy'
#     tensorboard_log: "../../../logs"

# MY GUESS CONFIG
# id: "guess"
# algo_config:
#     policy: 'MlpLstmPolicy'
#     tensorboard_log: "../../../logs"
#     batch_size: 64
#     n_steps: 1024
#     gae_lambda: 0.98
#     gamma: 0.995
#     use_sde: True

# # SLOW LEARN
# id: "slow"
# algo_config:
#     policy: 'MlpLstmPolicy'
#     tensorboard_log: "../../logs"
#     learning_rate: 0.0001
#     # default learning rate = 0.0003

# # EXTRA SLOW LEARN
# id: "extra-slow"
# algo_config:
#     policy: 'MlpLstmPolicy'
#     tensorboard_log: "../../logs"
#     learning_rate: 0.00003


# algo hyperpars taken from:
# https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo_lstm.yml


# # BIPEDAL WALKER
# id: "bipedal"
# algo_config:
#     # normalize: True # not clear what this one actually does -- from the source code it seems to 'activate' VecNormalize, but more care & examination needed
#     policy: 'MlpLstmPolicy'
#     tensorboard_log: "../../../logs"
#     n_steps: 256
#     batch_size: 256
#     gae_lambda: 0.95
#     gamma: 0.999
#     n_epochs: 10
#     ent_coef: 0.0
#     learning_rate: !!float 3e-4
#     clip_range: 0.18
#     policy_kwargs: "dict(
#                     ortho_init=False,
#                     activation_fn=torch.nn.ReLU,
#                     lstm_hidden_size=64,
#                     enable_critic_lstm=True,
#                     net_arch=dict(pi=[64], vf=[64])
#                   )"

# # HALF CHEETAH V4

# id: "cheetah"
# algo_config:
#     policy: 'MlpLstmPolicy'
#     tensorboard_log: "../../../logs"
#     batch_size: 64
#     n_steps: 512
#     gamma: 0.98
#     learning_rate: 2.0633e-05
#     ent_coef: 0.000401762
#     clip_range: 0.1
#     n_epochs: 20
#     gae_lambda: 0.92
#     max_grad_norm: 0.8
#     vf_coef: 0.58096
#     policy_kwargs: "dict(
#                     log_std_init=-2,
#                     ortho_init=False,
#                     activation_fn=torch.nn.ReLU,
#                     net_arch=dict(pi=[256, 256], vf=[256, 256])
#                   )"



# INVERTED PENDULUM
# id: "inv_pend"
# algo_config:
#     tensorboard_log: "../../../logs"
#     policy: 'MlpLstmPolicy'
#     n_steps: 2048
#     batch_size: 64
#     gae_lambda: 0.95
#     gamma: 0.99
#     n_epochs: 10
#     ent_coef: 0.0
#     learning_rate: 2.5e-4
#     clip_range: 0.2


# # MOUNTAIN CAR NO VEL

id: "MtCar-[256,128]"
algo_config:
    tensorboard_log: "../../../logs"
    policy: 'MlpLstmPolicy'
    batch_size: 256
    n_steps: 1024
    gamma: 0.9999
    learning_rate: !!float 7.77e-05
    ent_coef: 0.00429
    clip_range: 0.1
    n_epochs: 10
    gae_lambda: 0.9
    max_grad_norm: 5
    vf_coef: 0.19
    use_sde: True
    sde_sample_freq: 8
    policy_kwargs: "dict(log_std_init=-3.29, ortho_init=False,
                       lstm_hidden_size=64,
                       enable_critic_lstm=True,
                       net_arch=dict(pi=[256,128], vf=[256,128]))"

# SPACE INVADERS V4
# id: "space_invaders"
# algo_config:
#     tensorboard_log: "../../../logs"
#     policy: 'MlpLstmPolicy'
#     batch_size: 512
#     # clip_range: 0.1
#     ent_coef: 0.012
#     frame_stack: 4
#     learning_rate: 2.5e-4
#     policy_kwargs: dict(enable_critic_lstm=False, lstm_hidden_size=128, )
#     vf_coef: 0.5
